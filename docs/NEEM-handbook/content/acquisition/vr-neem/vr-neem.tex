\section{VR \neems}
\label{sec:vr-neem}
\lstset{style=lispcode}

This section will describe how \neems can be generated with a Virtual Reality setup and how they can be used to help a robot to perform everyday household activities. The core idea is essentially to show the robot how to do a specific task by demonstrating it within a Virtual Reality environment. VR also allows to know and record exactly where objects within the environment are located. The actions of the human user can be tracked via the VR-Equipment.  With the headset the general location of the human user within the VR-environment is acquired, as well as what the user is looking at. The VR motion controllers represent virtual hands, which the human user can open and close. Based on this, it is possible to generate data about which objects were grasped, how they were grasped, where was the human located when he interacted with certain objects and much more. 

In Virtual Reality it is possible to setup any environment imaginable a lot faster than in the real world. This means, that the real environment of a robot can be exactly replicated in Virtual Reality, or it can be slightly changed, or a completely new environment can be generated. Imagine how a kitchen almost never looks exactly the same in two households, even though it contains the same appliances like an oven or a shelf for the plates.  Virtual Reality allows us to setup many variations of a kitchen environment, which allows us to collect \neems of more variety and therefore make sure, that if the robot acts based on the collected data, it is not biased towards one very specific kitchen setup, but rather is able to find what is needed in a different kitchen also. 

These VR-\neems can be used within \cram plans to infer information about the environment. For example, questions like ``''Where was the human user standing when he was grasping the bowl?'', ``On which surface was the bowl placed?'' or ``With which hand did the human grasp the bowl?'' can all be answered by KnowRob, based on the knowledge which the VR-\neems provide. It is also important to take the relations between objects into account. For example, when setting up a breakfast table, the spoon will be placed to the right of the bowl on the table by the most users. It might not be that important to know where exactly the spoon is placed in terms of the exact coordinates, but much rather that it is always placed relative to the bowl and a certain distance away from this bowl. Or that the bowl is placed on the table near an edge rather than in the middle of the table. 

The following section will go into more detail on how exactly one can generate VR-\neems, what needs to be installed to be able to do so, and how they can be used within \cram.

\subsection{Prerequisite}
%Everythhing needed to be able to record NEEMs. Unreal, Plugins, Kitchen, RobCog, robcog_knowrob (for quering). Refer to RobCog and some of the CRAM-VR Tutorials. Also setup kitchen with all the arrays/Links.
Before VR-\neem generation can take place, the proper VR-environment needs to be set up within the Unreal Engine, including the installation of the USemLog Plugin, which records the \neems and generates the appropriate .owl files\todo{Seba: Are you using the newest KnowRob with Soma? Alina: not yet. The plugin needs to be adapted as well as the knowrob_robcog package}. The plugin and a setup of a kitchen environment can be found within the RobCog project. 
The KnowRob and MongoDB installation are the same as in the section above. \todo{@Alina add links to the sections}
In order to be able to use the \neems within \cram, the data first needs to be transferred into the MongoDB and KnowRob. This can be achieved by running the \textit{vr neems to knowrob} scripts. Please refer to the README.md for execution examples.

 
To summarize, you will need to install the following components: 
\begin{itemize}
	\item Unreal Engine\footnote{\url{https://www.unrealengine.com/}} Version 4.22.3
	\item RobCog (under adaptation) \footnote{\url{https://github.com/robcog-iai/RobCoG}}
	\item vr\_neems\_to\_knowrob\footnote{\url{https://github.com/ease-crc/vr_neems_to_knowrob}} 
	\item knowrob\_robcog (under construction) \todo{@Alina add link}
	\item CRAM\footnote{\url{https://github.com/cram2/cram.git}} Branch: boxy-melodic

\end{itemize}

\subsection{Recording Virtual Reality Narrative Enabled Episodic Memories}
%Run around in VR and do stuff, dump semantic map using plug in
\todo{@Alina add screenshots}
\todo{Seba: can you describe or set a link to tutorials how to familiar with the vr system ? }
Check if all items that you want to appear in the \neems, have a tag under which they will be represented within the ontology. You can check the tag by clicking any item within the kitchen environment, and going to the \textit{Actor} section in the \textit{details} pane. Click the little arrow to expand the section, and also expand the \textit{Tags} section. There you should see something like this:

\begin{lstlisting}

SemLog;Class,IAIIslandArea;Id,tpzV6l885UGL785BwZFHYQ;

\end{lstlisting}
This string defines the class of the item and it's id. More information can be added here, depending on the item and the ontology and the level of detail desired within the classification. This is also very important when introducing new items to the Virtual Reality setup. Items which do not have these tags, will not be included in the recorded \neem data. 

Once everything is set up and all potentially new items are within the virtual reality environment, the recording can begin. First, the semantic map can be automatically generated by going to the \textit{RobCog} pane within the \textit{Unreal Engine} editor, and clicking the \textit{SemanticMap} button. 
\todo{@Alina add Screenshot}
After this, the generated semantic map should be located in the \textit{RobCog/Episodes} directory. It contains the initial state of the virtual reality environment, including the position and rotation of all the furniture objects and also their classifications. 

The next step is then to start the virtual reality simulation and perform some actions within it, e.g. setting up a breakfast table. The position of the VR-headset and controllers is being tracked the entire time, as well as the interactions of the virtual hands with the environment and objects. Picking up an object would generate a \textit{GraspingSomething}-action. Placing an object down on a table would generate a \textit{Contact}-Action between the object and the surface it has been placed upon. All these interactions can later on be queried for. 

Once all the desired actions are complete, the simulation can be stopped and an \textit{EventData\_ID} directory appears in the \textit{RobCog/Episodes} directory. It contains and \textit{EventData\_ID.owl} file, an \textit{EventData\_ID.html} file, which visualizes all the occurred events, and a \textit{RawData\_ID.json}, which contains all the information about the performed actions and events. The last file is the one that needs to be uploaded into the \textit{MongoDB}. 


\subsection{Transferring VR-NEEMs into the Knowledge Base}
%Use the scripts. Maybe put the scripts into a different repo. Maybe RobCog? -> make own knowrob_robcog fork and put it there.
Please refer to the README of these scripts \url{https://github.com/ease-crc/vr_neems_to_knowrob} in order to import the VR-\neems into KnowRob and MongoDB. More information about the import, how it generally works and why the scripts were created the way they are, please refer to: \url{http://cram-system.org/tutorials/advanced/unreal#importing_new_episode_data_into_mongodb_and_knowrob_additional_information}. 

\subsection{Using VR-NEEM Data in CRAM plans}
%Which data is used, how is it used/sampled. Add reference to Paper maybe
There is a demo within \cram which uses the data collected in VR, including a tutorial on how to run it. It can be found here: \url{http://cram-system.org/tutorials/advanced/unreal}. In this demo the robot performs a pick and place task, picking up a cup, bowl and a spoon from the sink counter, and placing them onto the kitchen island. In order to do this, \cram queries KnowRob for the following information: 

\begin{itemize}
	\item where/from which surface was the object picked up?
	\item where was the human user standing when he was picking up/placing the object?
	\item on which surface and where was the object placed? (In relation to other objects)
	\item with which hand was the object grasped?
	\item from which direction (top,left,right...) was the object grasped?
\end{itemize}

Since the virtual reality kitchen can look very different than the one the robot is acting in, all of the poses are calculated relative to the respective surfaces and each other. For example, the spoon is always placed to the right of the bowl. 

For more information on how \cram interacts with KnowRob and how json-prolog can be used within \cram, please refer to \url{http://cram-system.org/tutorials/intermediate/json_prolog}




%\subsection{Future Work}
%references to future work, aka. my masters thesis and ML to showcase how else VR-NEEms could be used.

