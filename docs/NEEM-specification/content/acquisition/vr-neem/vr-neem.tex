\section{VR \neems}
\label{sec:vr-neem}
\lstset{style=lispcode}

This section will describe how \neems can be generated within a Virtual Reality environment and how they can be utilized within \cram plans to help a robot perform everyday household activities. The use of VR allows us as humans to show the robot an action we want it to perform within a variety of different environments. This facilitates learning of  a lot of common sense knowledge, e.g. where the objects necessary to perform a specific action are commonly stored within the environment, which objects are needed for a specific action, where the human user was standing when he was grasping a certain object, how the objects were arranged on a surface relative to one another and how the human user grasped them.  

The example scenario used here is the breakfast setting scenario. This means that the robot is supposed to set up the table with a bowl, cup and a spoon in preparation of a breakfast cereal meal. \todo{@Alina: move this probably to a different section.}
% Maybe make a section describing all the advantages it brings to use VR? But maybe this is also enough

\subsection{Prerequisite}
%Everythhing needed to be able to record NEEMs. Unreal, Plugins, Kitchen, RobCog, robcog_knowrob (for quering). Refer to RobCog and some of the CRAM-VR Tutorials. Also setup kitchen with all the arrays/Links.
Before VR-\neem generation can take place, the proper VR-environment needs to be set up within the Unreal Engine, including the installation of the USemLog Plugin, which records the \neems and generates the appropriate .owl files. The plugin and a setup of a kitchen environment can be found within the RobCog project. 
The KnowRob and MongoDB installation are the same as in the section above. \todo{@Alina add links to the sections}
In order to be able to use the \neems within \cram, the data first needs to be transferred into the MongoDB and KnowRob. This can be achieved by running the \textit{vr neems to knowrob} scripts. Please refer to the README.md for execution examples.

 
To summarize: 
\begin{itemize}
	\item Unreal Engine\footnote{\url{https://www.unrealengine.com/}} Version 4.22.3
	\item RobCog\footnote{\url{https://github.com/robcog-iai/RobCoG}}
	\item vr\_neems\_to\_knowrob\footnote{\url{https://github.com/ease-crc/vr_neems_to_knowrob}} 
	\item knowrob\_robcog \todo{@Alina add link}
	\item CRAM\footnote{\url{https://github.com/cram2/cram.git}} Branch: boxy-melodic

\end{itemize}

\subsection{Recording Virtual Reality Narrative Enabled Episodic Memories}
%Run around in VR and do stuff, dump semantic map using plug in
\todo{@Alina add screenshots}
Check if all items that you want to appear in the \neems, have a tag under which they will be represented within the ontology. You can check the tag by clicking any item within the kitchen environment, and going to the \textit{Actor} section in the \textit{details} pane. Click the little arrow to expand the section, and also expand the \textit{Tags} section. There you should see something like this:

\begin{lstlisting}

SemLog;Class,IAIIslandArea;Id,tpzV6l885UGL785BwZFHYQ;

\end{lstlisting}
This string defines the class of the item and it's id. More information can be added here, depending on the item and the ontology and the level of detail desired within the classification. This is also very important when introducing new items to the Virtual Reality setup. Items which do not have these tags, will not be included in the recorded \neem data. 

Once everything is set up and all potentially new items are within the virtual reality environment, the recording can begin. First, the semantic map can be automatically generated by going to the \textit{RobCog} pane within the \textit{Unreal Engine} editor, and clicking the \textit{SemanticMap} button. 
\todo{@Alina add Screenshot}
After this, the generated semantic map should be located in the \textit{RobCog/Episodes} directory. It contains the initial state of the virtual reality environment, including the position and rotation of all the furniture objects and also their classifications. 

The next step is then to start the virtual reality simulation and perform some actions within it, e.g. setting up a breakfast table. The position of the VR-headset and controllers is being tracked the entire time, as well as the interactions of the virtual hands with the environment and objects. Picking up an object would generate a \textit{GraspingSomething}-action. Placing an object down on a table would generate a \textit{Contact}-Action between the object and the surface it has been placed upon. All these interactions can later on be queried for. 

Once all the desired actions are complete, the simulation can be stopped and an \textit{EventData\_ID} directory appears in the \textit{RobCog/Episodes} directory. It contains and \textit{EventData\_ID.owl} file, an \textit{EventData\_ID.html} file, which visualizes all the occurred events, and a \textit{RawData\_ID.json}, which contains all the information about the performed actions and events. The last file is the one that needs to be uploaded into the \textit{MongoDB}. 


\subsection{Transferring VR-NEEMs into the Knowledge Base}
%Use the scripts. Maybe put the scripts into a different repo. Maybe RobCog? -> make own knowrob_robcog fork and put it there.
Please refer to the README of these scripts \url{https://github.com/ease-crc/vr_neems_to_knowrob} in order to import the VR-\neems into KnowRob and MongoDB. More information about the import, how it generally works and why the scripts were created the way they are, please refer to: \url{http://cram-system.org/tutorials/advanced/unreal#importing_new_episode_data_into_mongodb_and_knowrob_additional_information}. 

\subsection{Using VR-NEEM Data in CRAM plans}
%Which data is used, how is it used/sampled. Add reference to Paper maybe
There is a demo within \cram which uses the data collected in VR, including a tutorial on how to run it. It can be found here: \url{http://cram-system.org/tutorials/advanced/unreal}. In this demo the robot performs a pick and place task, picking up a cup, bowl and a spoon from the sink counter, and placing them onto the kitchen island. In order to do this, \cram queries KnowRob for the following information: 

\begin{itemize}
	\item where/from which surface was the object picked up?
	\item where was the human user standing when he was picking up/placing the object?
	\item on which surface and where was the object placed? (In relation to other objects)
	\item with which hand was the object grasped?
	\item from which direction (top,left,right...) was the object grasped?
\end{itemize}

Since the virtual reality kitchen can look very different than the one the robot is acting in, all of the poses are calculated relative to the respective surfaces and each other. For example, the spoon is always placed to the right of the bowl. 

For more information on how \cram interacts with KnowRob and how json-prolog can be used within \cram, please refer to \url{http://cram-system.org/tutorials/intermediate/json_prolog}




\subsection{Future Work}
%references to future work, aka. my masters thesis and ML to showcase how else VR-NEEms could be used.

